"""
Parallel Orchestrator PR Reviewer
==================================

PR reviewer using Claude Agent SDK subagents for parallel specialist analysis.

The orchestrator analyzes the PR and delegates to specialized agents (security,
quality, logic, codebase-fit, ai-triage) which run in parallel. Results are
synthesized into a final verdict.

Key Design:
- AI decides which agents to invoke (NOT programmatic rules)
- Subagents defined via SDK `agents={}` parameter
- SDK handles parallel execution automatically
- User-configured model from frontend settings (no hardcoding)
"""

from __future__ import annotations

import hashlib
import logging
import os
from collections import defaultdict
from enum import Enum
from pathlib import Path
from typing import Any

from claude_agent_sdk import AgentDefinition

try:
    from ...core.client import create_client
    from ...phase_config import get_thinking_budget, resolve_model_id
    from ..context_gatherer import PRContext, PRContextGatherer, _validate_git_ref
    from ..gh_client import GHClient
    from ..models import (
        BRANCH_BEHIND_BLOCKER_MSG,
        BRANCH_BEHIND_REASONING,
        GitHubRunnerConfig,
        MergeVerdict,
        PRReviewFinding,
        PRReviewResult,
        ReviewSeverity,
    )
    from .category_utils import map_category
    from .io_utils import safe_print
    from .pr_worktree_manager import PRWorktreeManager
    from .pydantic_models import AgentAgreement, ParallelOrchestratorResponse
    from .sdk_utils import process_sdk_stream
except (ImportError, ValueError, SystemError):
    from context_gatherer import PRContext, PRContextGatherer, _validate_git_ref
    from core.client import create_client
    from gh_client import GHClient
    from models import (
        BRANCH_BEHIND_BLOCKER_MSG,
        BRANCH_BEHIND_REASONING,
        GitHubRunnerConfig,
        MergeVerdict,
        PRReviewFinding,
        PRReviewResult,
        ReviewSeverity,
    )
    from phase_config import get_thinking_budget, resolve_model_id
    from services.category_utils import map_category
    from services.io_utils import safe_print
    from services.pr_worktree_manager import PRWorktreeManager
    from services.pydantic_models import AgentAgreement, ParallelOrchestratorResponse
    from services.sdk_utils import process_sdk_stream


logger = logging.getLogger(__name__)

# Check if debug mode is enabled
DEBUG_MODE = os.environ.get("DEBUG", "").lower() in ("true", "1", "yes")

# Directory for PR review worktrees (inside github/pr for consistency)
PR_WORKTREE_DIR = ".auto-claude/github/pr/worktrees"


class ConfidenceTier(str, Enum):
    """Confidence tiers for finding routing.

    Findings are routed based on their confidence score:
    - HIGH (>=0.8): Included as-is
    - MEDIUM (0.5-0.8): Included with "[Potential]" prefix
    - LOW (<0.5): Logged but excluded from output
    """

    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

    # Thresholds (class-level constants)
    @classmethod
    def get_tier(cls, confidence: float) -> ConfidenceTier:
        """Get tier for a given confidence value."""
        if confidence >= 0.8:  # HIGH_THRESHOLD
            return cls.HIGH
        elif confidence >= 0.5:  # LOW_THRESHOLD
            return cls.MEDIUM
        else:
            return cls.LOW


def _validate_finding_evidence(finding: PRReviewFinding) -> tuple[bool, str]:
    """
    Check if finding has actual code evidence, not just descriptions.

    Returns:
        Tuple of (is_valid, reason)
    """
    if not finding.evidence:
        return False, "No evidence provided"

    evidence = finding.evidence.strip()
    if len(evidence) < 10:
        return False, "Evidence too short (< 10 chars)"

    # Reject generic descriptions that aren't code
    description_patterns = [
        "the code",
        "this function",
        "it appears",
        "seems to",
        "may be",
        "could be",
        "might be",
        "appears to",
        "there is",
        "there are",
    ]
    evidence_lower = evidence.lower()
    for pattern in description_patterns:
        if evidence_lower.startswith(pattern):
            return False, f"Evidence starts with description pattern: '{pattern}'"

    # Evidence should look like code (has syntax characters)
    code_chars = [
        "=",
        "(",
        ")",
        "{",
        "}",
        ";",
        ":",
        ".",
        "->",
        "::",
        "[",
        "]",
        "'",
        '"',
    ]
    has_code_syntax = any(char in evidence for char in code_chars)

    if not has_code_syntax:
        return False, "Evidence lacks code syntax characters"

    return True, "Valid evidence"


def _is_finding_in_scope(
    finding: PRReviewFinding,
    changed_files: list[str],
) -> tuple[bool, str]:
    """
    Check if finding is within PR scope.

    Args:
        finding: The finding to check
        changed_files: List of file paths changed in the PR

    Returns:
        Tuple of (is_in_scope, reason)
    """
    if not finding.file:
        return False, "No file specified"

    # Check if file is in changed files
    if finding.file not in changed_files:
        # Allow impact findings (about how changes affect other files)
        impact_keywords = ["breaks", "affects", "impact", "caller", "depends"]
        description_lower = (finding.description or "").lower()
        is_impact_finding = any(kw in description_lower for kw in impact_keywords)

        if not is_impact_finding:
            return (
                False,
                f"File '{finding.file}' not in PR changed files and not an impact finding",
            )

    # Check line number is reasonable (> 0)
    if finding.line is not None and finding.line <= 0:
        return False, f"Invalid line number: {finding.line}"

    return True, "In scope"


class ParallelOrchestratorReviewer:
    """
    PR reviewer using SDK subagents for parallel specialist analysis.

    The orchestrator:
    1. Analyzes the PR (size, complexity, file types, risk areas)
    2. Delegates to appropriate specialist agents (SDK handles parallel execution)
    3. Synthesizes findings into a final verdict

    Model Configuration:
    - Orchestrator uses user-configured model from frontend settings
    - Specialist agents use model="inherit" (same as orchestrator)
    """

    def __init__(
        self,
        project_dir: Path,
        github_dir: Path,
        config: GitHubRunnerConfig,
        progress_callback=None,
    ):
        self.project_dir = Path(project_dir)
        self.github_dir = Path(github_dir)
        self.config = config
        self.progress_callback = progress_callback
        self.worktree_manager = PRWorktreeManager(project_dir, PR_WORKTREE_DIR)

    def _report_progress(self, phase: str, progress: int, message: str, **kwargs):
        """Report progress if callback is set."""
        if self.progress_callback:
            import sys

            if "orchestrator" in sys.modules:
                ProgressCallback = sys.modules["orchestrator"].ProgressCallback
            else:
                try:
                    from ..orchestrator import ProgressCallback
                except ImportError:
                    from orchestrator import ProgressCallback

            self.progress_callback(
                ProgressCallback(
                    phase=phase, progress=progress, message=message, **kwargs
                )
            )

    def _load_prompt(self, filename: str) -> str:
        """Load a prompt file from the prompts/github directory."""
        prompt_file = (
            Path(__file__).parent.parent.parent.parent / "prompts" / "github" / filename
        )
        if prompt_file.exists():
            return prompt_file.read_text(encoding="utf-8")
        logger.warning(f"Prompt file not found: {prompt_file}")
        return ""

    def _create_pr_worktree(self, head_sha: str, pr_number: int) -> Path:
        """Create a temporary worktree at the PR head commit.

        Args:
            head_sha: The commit SHA of the PR head (validated before use)
            pr_number: The PR number for naming

        Returns:
            Path to the created worktree

        Raises:
            RuntimeError: If worktree creation fails
            ValueError: If head_sha fails validation (command injection prevention)
        """
        # SECURITY: Validate git ref before use in subprocess calls
        if not _validate_git_ref(head_sha):
            raise ValueError(
                f"Invalid git ref: '{head_sha}'. "
                "Must contain only alphanumeric characters, dots, slashes, underscores, and hyphens."
            )

        return self.worktree_manager.create_worktree(head_sha, pr_number)

    def _cleanup_pr_worktree(self, worktree_path: Path) -> None:
        """Remove a temporary PR review worktree with fallback chain.

        Args:
            worktree_path: Path to the worktree to remove
        """
        self.worktree_manager.remove_worktree(worktree_path)

    def _cleanup_stale_pr_worktrees(self) -> None:
        """Clean up orphaned, expired, and excess PR review worktrees on startup."""
        stats = self.worktree_manager.cleanup_worktrees()
        if stats["total"] > 0:
            logger.info(
                f"[PRReview] Cleanup: removed {stats['total']} worktrees "
                f"(orphaned={stats['orphaned']}, expired={stats['expired']}, excess={stats['excess']})"
            )

    def _define_specialist_agents(self) -> dict[str, AgentDefinition]:
        """
        Define specialist agents for the SDK.

        Each agent has:
        - description: When the orchestrator should invoke this agent
        - prompt: System prompt for the agent
        - tools: Tools the agent can use (read-only for PR review)
        - model: "inherit" = use same model as orchestrator (user's choice)

        Returns AgentDefinition dataclass instances as required by the SDK.
        """
        # Load agent prompts from files
        security_prompt = self._load_prompt("pr_security_agent.md")
        quality_prompt = self._load_prompt("pr_quality_agent.md")
        logic_prompt = self._load_prompt("pr_logic_agent.md")
        codebase_fit_prompt = self._load_prompt("pr_codebase_fit_agent.md")
        ai_triage_prompt = self._load_prompt("pr_ai_triage.md")
        validator_prompt = self._load_prompt("pr_finding_validator.md")

        return {
            "security-reviewer": AgentDefinition(
                description=(
                    "Security specialist. Use for OWASP Top 10, authentication, "
                    "injection, cryptographic issues, and sensitive data exposure. "
                    "Invoke when PR touches auth, API endpoints, user input, database queries, "
                    "or file operations."
                ),
                prompt=security_prompt
                or "You are a security expert. Find vulnerabilities.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "quality-reviewer": AgentDefinition(
                description=(
                    "Code quality expert. Use for complexity, duplication, error handling, "
                    "maintainability, and pattern adherence. Invoke when PR has complex logic, "
                    "large functions, or significant business logic changes."
                ),
                prompt=quality_prompt
                or "You are a code quality expert. Find quality issues.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "logic-reviewer": AgentDefinition(
                description=(
                    "Logic and correctness specialist. Use for algorithm verification, "
                    "edge cases, state management, and race conditions. Invoke when PR has "
                    "algorithmic changes, data transformations, concurrent operations, or bug fixes."
                ),
                prompt=logic_prompt
                or "You are a logic expert. Find correctness issues.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "codebase-fit-reviewer": AgentDefinition(
                description=(
                    "Codebase consistency expert. Use for naming conventions, ecosystem fit, "
                    "architectural alignment, and avoiding reinvention. Invoke when PR introduces "
                    "new patterns, large additions, or code that might duplicate existing functionality."
                ),
                prompt=codebase_fit_prompt
                or "You are a codebase expert. Check for consistency.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "ai-triage-reviewer": AgentDefinition(
                description=(
                    "AI comment validator. Use for triaging comments from CodeRabbit, "
                    "Gemini Code Assist, Cursor, Greptile, and other AI reviewers. "
                    "Invoke when PR has existing AI review comments that need validation."
                ),
                prompt=ai_triage_prompt
                or "You are an AI triage expert. Validate AI comments.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "finding-validator": AgentDefinition(
                description=(
                    "Finding validation specialist. Re-investigates findings to validate "
                    "they are actually real issues, not false positives. "
                    "Reads the ACTUAL CODE at the finding location with fresh eyes. "
                    "CRITICAL: Invoke for ALL findings after specialist agents complete. "
                    "Can confirm findings as valid OR dismiss them as false positives."
                ),
                prompt=validator_prompt
                or "You validate whether findings are real issues.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
        }

    def _build_orchestrator_prompt(self, context: PRContext) -> str:
        """Build full prompt for orchestrator with PR context."""
        # Load orchestrator prompt
        base_prompt = self._load_prompt("pr_parallel_orchestrator.md")
        if not base_prompt:
            base_prompt = "You are a PR reviewer. Analyze and delegate to specialists."

        # Build file list
        files_list = []
        for file in context.changed_files:
            files_list.append(
                f"- `{file.path}` (+{file.additions}/-{file.deletions}) - {file.status}"
            )

        # Build composite diff
        patches = []
        MAX_DIFF_CHARS = 200_000

        for file in context.changed_files:
            if file.patch:
                patches.append(f"\n### File: {file.path}\n{file.patch}")

        diff_content = "\n".join(patches)

        if len(diff_content) > MAX_DIFF_CHARS:
            diff_content = diff_content[:MAX_DIFF_CHARS] + "\n\n... (diff truncated)"

        # Build AI comments context if present (with timestamps for timeline awareness)
        ai_comments_section = ""
        if context.ai_bot_comments:
            ai_comments_list = []
            for comment in context.ai_bot_comments[:20]:
                ai_comments_list.append(
                    f"- **{comment.tool_name}** ({comment.created_at}) on {comment.file or 'general'}: "
                    f"{comment.body[:200]}..."
                )
            ai_comments_section = f"""
### AI Review Comments (need triage)
Found {len(context.ai_bot_comments)} comments from AI tools.
**IMPORTANT: Check timestamps! If a later commit fixed an AI-flagged issue, use ADDRESSED verdict (not FALSE_POSITIVE).**

{chr(10).join(ai_comments_list)}
"""

        # Build commits timeline section (important for AI triage)
        commits_section = ""
        if context.commits:
            commits_list = []
            for commit in context.commits:
                sha = commit.get("oid", "")[:8]
                message = commit.get("messageHeadline", "")
                committed_at = commit.get("committedDate", "")
                commits_list.append(f"- `{sha}` ({committed_at}): {message}")
            commits_section = f"""
### Commit Timeline
{chr(10).join(commits_list)}
"""

        pr_context = f"""
---

## PR Context for Review

**PR Number:** {context.pr_number}
**Title:** {context.title}
**Author:** {context.author}
**Base:** {context.base_branch} â† **Head:** {context.head_branch}
**Files Changed:** {len(context.changed_files)} files
**Total Changes:** +{context.total_additions}/-{context.total_deletions} lines

### Description
{context.description}

### All Changed Files
{chr(10).join(files_list)}
{commits_section}{ai_comments_section}
### Code Changes
```diff
{diff_content}
```

---

Now analyze this PR and delegate to the appropriate specialist agents.
Remember: YOU decide which agents to invoke based on YOUR analysis.
The SDK will run invoked agents in parallel automatically.
"""

        return base_prompt + pr_context

    def _create_sdk_client(
        self, project_root: Path, model: str, thinking_budget: int | None
    ):
        """Create SDK client with subagents and configuration.

        Args:
            project_root: Root directory of the project
            model: Model to use for orchestrator
            thinking_budget: Max thinking tokens budget

        Returns:
            Configured SDK client instance
        """
        return create_client(
            project_dir=project_root,
            spec_dir=self.github_dir,
            model=model,
            agent_type="pr_orchestrator_parallel",
            max_thinking_tokens=thinking_budget,
            agents=self._define_specialist_agents(),
            output_format={
                "type": "json_schema",
                "schema": ParallelOrchestratorResponse.model_json_schema(),
            },
        )

    def _extract_structured_output(
        self, structured_output: dict[str, Any] | None, result_text: str
    ) -> tuple[list[PRReviewFinding], list[str]]:
        """Parse and extract findings from structured output or text fallback.

        Args:
            structured_output: Structured JSON output from agent
            result_text: Raw text output as fallback

        Returns:
            Tuple of (findings list, agents_invoked list)
        """
        agents_from_structured: list[str] = []

        if structured_output:
            findings, agents_from_structured = self._parse_structured_output(
                structured_output
            )
            if findings is None and result_text:
                findings = self._parse_text_output(result_text)
            elif findings is None:
                findings = []
        else:
            findings = self._parse_text_output(result_text)

        return findings, agents_from_structured

    def _log_agents_invoked(self, agents: list[str]) -> None:
        """Log invoked agents with clear formatting.

        Args:
            agents: List of agent names that were invoked
        """
        if agents:
            safe_print(
                f"[ParallelOrchestrator] Specialist agents invoked: {', '.join(agents)}",
                flush=True,
            )
            for agent in agents:
                safe_print(f"[Agent:{agent}] Analysis complete")

    def _log_findings_summary(self, findings: list[PRReviewFinding]) -> None:
        """Log findings summary for verification.

        Args:
            findings: List of findings to summarize
        """
        if findings:
            safe_print(
                f"[ParallelOrchestrator] Parsed {len(findings)} findings from structured output",
                flush=True,
            )
            safe_print("[ParallelOrchestrator] Findings summary:")
            for i, f in enumerate(findings, 1):
                safe_print(
                    f"  [{f.severity.value.upper()}] {i}. {f.title} ({f.file}:{f.line})",
                    flush=True,
                )

    def _create_finding_from_structured(self, finding_data: Any) -> PRReviewFinding:
        """Create a PRReviewFinding from structured output data.

        Args:
            finding_data: Finding data from structured output

        Returns:
            PRReviewFinding instance
        """
        finding_id = hashlib.md5(
            f"{finding_data.file}:{finding_data.line}:{finding_data.title}".encode(),
            usedforsecurity=False,
        ).hexdigest()[:12]

        category = map_category(finding_data.category)

        try:
            severity = ReviewSeverity(finding_data.severity.lower())
        except ValueError:
            severity = ReviewSeverity.MEDIUM

        return PRReviewFinding(
            id=finding_id,
            file=finding_data.file,
            line=finding_data.line,
            title=finding_data.title,
            description=finding_data.description,
            category=category,
            severity=severity,
            suggested_fix=finding_data.suggested_fix or "",
            evidence=finding_data.evidence,
        )

    async def review(self, context: PRContext) -> PRReviewResult:
        """
        Main review entry point.

        Args:
            context: Full PR context with all files and patches

        Returns:
            PRReviewResult with findings and verdict
        """
        logger.info(
            f"[ParallelOrchestrator] Starting review for PR #{context.pr_number}"
        )

        # Clean up any stale worktrees from previous runs
        self._cleanup_stale_pr_worktrees()

        # Track worktree for cleanup
        worktree_path: Path | None = None

        try:
            self._report_progress(
                "orchestrating",
                35,
                "Parallel orchestrator analyzing PR...",
                pr_number=context.pr_number,
            )

            # Create temporary worktree at PR head commit for isolated review
            # This MUST happen BEFORE building the prompt so we can find related files
            # that exist in the PR but not in the current checkout
            head_sha = context.head_sha or context.head_branch

            if DEBUG_MODE:
                safe_print(
                    f"[PRReview] DEBUG: context.head_sha='{context.head_sha}'",
                    flush=True,
                )
                safe_print(
                    f"[PRReview] DEBUG: context.head_branch='{context.head_branch}'",
                    flush=True,
                )
                safe_print(f"[PRReview] DEBUG: resolved head_sha='{head_sha}'")

            # SECURITY: Validate the resolved head_sha (whether SHA or branch name)
            # This catches invalid refs early before subprocess calls
            if head_sha and not _validate_git_ref(head_sha):
                logger.warning(
                    f"[ParallelOrchestrator] Invalid git ref '{head_sha}', "
                    "using current checkout for safety"
                )
                head_sha = None

            if not head_sha:
                if DEBUG_MODE:
                    safe_print("[PRReview] DEBUG: No head_sha - using fallback")
                logger.warning(
                    "[ParallelOrchestrator] No head_sha available, using current checkout"
                )
                # Fallback to original behavior if no SHA available
                project_root = (
                    self.project_dir.parent.parent
                    if self.project_dir.name == "backend"
                    else self.project_dir
                )
            else:
                if DEBUG_MODE:
                    safe_print(
                        f"[PRReview] DEBUG: Creating worktree for head_sha={head_sha}",
                        flush=True,
                    )
                try:
                    worktree_path = self._create_pr_worktree(
                        head_sha, context.pr_number
                    )
                    project_root = worktree_path
                    # Count files in worktree to give user visibility (with limit to avoid slowdown)
                    MAX_FILE_COUNT = 10000
                    try:
                        file_count = 0
                        for f in worktree_path.rglob("*"):
                            if f.is_file() and ".git" not in f.parts:
                                file_count += 1
                                if file_count >= MAX_FILE_COUNT:
                                    break
                    except (OSError, PermissionError):
                        file_count = 0
                    file_count_str = (
                        f"{file_count:,}+"
                        if file_count >= MAX_FILE_COUNT
                        else f"{file_count:,}"
                    )
                    # Always log worktree creation with file count (not gated by DEBUG_MODE)
                    safe_print(
                        f"[PRReview] Created temporary worktree: {worktree_path.name} ({file_count_str} files)",
                        flush=True,
                    )
                    safe_print(
                        f"[PRReview] Worktree contains PR branch HEAD: {head_sha[:8]}",
                        flush=True,
                    )
                except (RuntimeError, ValueError) as e:
                    if DEBUG_MODE:
                        safe_print(
                            f"[PRReview] DEBUG: Worktree creation FAILED: {e}",
                            flush=True,
                        )
                    logger.warning(
                        f"[ParallelOrchestrator] Worktree creation failed, "
                        f"using current checkout: {e}"
                    )
                    # Fallback to original behavior if worktree creation fails
                    project_root = (
                        self.project_dir.parent.parent
                        if self.project_dir.name == "backend"
                        else self.project_dir
                    )

            # Rescan for related files using the worktree/project root
            # This fixes the issue where related files were 0 because context gathering
            # happened BEFORE the worktree was created (PR files didn't exist locally)
            if context.changed_files:
                new_related_files = PRContextGatherer.find_related_files_for_root(
                    context.changed_files,
                    project_root,
                )
                # Always log rescan result (not gated by DEBUG_MODE)
                if new_related_files:
                    context.related_files = new_related_files
                    safe_print(
                        f"[PRReview] Rescanned in worktree: found {len(new_related_files)} related files"
                    )
                else:
                    safe_print(
                        f"[PRReview] Rescanned in worktree: found 0 related files "
                        f"(initial scan found {len(context.related_files)})"
                    )

            # Build orchestrator prompt AFTER worktree creation and related files rescan
            prompt = self._build_orchestrator_prompt(context)

            # Use model and thinking level from config (user settings)
            # Resolve model shorthand via environment variable override if configured
            model_shorthand = self.config.model or "sonnet"
            model = resolve_model_id(model_shorthand)
            thinking_level = self.config.thinking_level or "medium"
            thinking_budget = get_thinking_budget(thinking_level)

            logger.info(
                f"[ParallelOrchestrator] Using model={model}, "
                f"thinking_level={thinking_level}, thinking_budget={thinking_budget}"
            )

            # Create client with subagents defined
            # SDK handles parallel execution when Claude invokes multiple Task tools
            client = self._create_sdk_client(project_root, model, thinking_budget)

            self._report_progress(
                "orchestrating",
                40,
                "Orchestrator delegating to specialist agents...",
                pr_number=context.pr_number,
            )

            # Run orchestrator session using shared SDK stream processor
            async with client:
                await client.query(prompt)

                safe_print(
                    f"[ParallelOrchestrator] Running orchestrator ({model})...",
                    flush=True,
                )

                # Process SDK stream with shared utility
                stream_result = await process_sdk_stream(
                    client=client,
                    context_name="ParallelOrchestrator",
                    model=model,
                )

                # Check for stream processing errors
                if stream_result.get("error"):
                    logger.error(
                        f"[ParallelOrchestrator] SDK stream failed: {stream_result['error']}"
                    )
                    raise RuntimeError(
                        f"SDK stream processing failed: {stream_result['error']}"
                    )

                result_text = stream_result["result_text"]
                structured_output = stream_result["structured_output"]
                agents_invoked = stream_result["agents_invoked"]
                msg_count = stream_result["msg_count"]

            self._report_progress(
                "finalizing",
                50,
                "Synthesizing findings...",
                pr_number=context.pr_number,
            )

            # Parse findings from output (structured output also returns agents)
            findings, agents_from_structured = self._extract_structured_output(
                structured_output, result_text
            )

            # Use agents from structured output (more reliable than streaming detection)
            final_agents = (
                agents_from_structured if agents_from_structured else agents_invoked
            )
            logger.info(
                f"[ParallelOrchestrator] Session complete. Agents invoked: {final_agents}"
            )
            safe_print(
                f"[ParallelOrchestrator] Complete. Agents invoked: {final_agents}",
                flush=True,
            )

            # Deduplicate findings
            unique_findings = self._deduplicate_findings(findings)

            # Cross-validate findings: boost confidence when multiple agents agree
            cross_validated_findings, agent_agreement = self._cross_validate_findings(
                unique_findings
            )

            # Log cross-validation results
            logger.info(
                f"[PRReview] Cross-validation: {len(agent_agreement.agreed_findings)} multi-agent, "
                f"{len(cross_validated_findings) - len(agent_agreement.agreed_findings)} single-agent"
            )

            # Log full agreement details at debug level for monitoring
            logger.debug(
                f"[PRReview] AgentAgreement: {agent_agreement.model_dump_json()}"
            )

            # Apply programmatic evidence and scope filters
            # These catch edge cases that slip through the finding-validator
            changed_file_paths = [f.path for f in context.changed_files]
            validated_findings = []
            filtered_findings = []

            for finding in cross_validated_findings:
                # Check evidence quality
                evidence_valid, evidence_reason = _validate_finding_evidence(finding)
                if not evidence_valid:
                    logger.info(
                        f"[PRReview] Filtered finding {finding.id}: {evidence_reason}"
                    )
                    filtered_findings.append((finding, evidence_reason))
                    continue

                # Check scope
                scope_valid, scope_reason = _is_finding_in_scope(
                    finding, changed_file_paths
                )
                if not scope_valid:
                    logger.info(
                        f"[PRReview] Filtered finding {finding.id}: {scope_reason}"
                    )
                    filtered_findings.append((finding, scope_reason))
                    continue

                validated_findings.append(finding)

            logger.info(
                f"[PRReview] Findings: {len(validated_findings)} valid, "
                f"{len(filtered_findings)} filtered"
            )

            # Apply confidence routing to filter low-confidence findings
            # and mark medium-confidence findings with "[Potential]" prefix
            routed_findings = self._apply_confidence_routing(validated_findings)

            logger.info(
                f"[PRReview] Confidence routing: {len(routed_findings)} included, "
                f"{len(validated_findings) - len(routed_findings)} dropped (low confidence)"
            )

            # Use routed findings for verdict and summary
            unique_findings = routed_findings

            logger.info(
                f"[ParallelOrchestrator] Review complete: {len(unique_findings)} findings"
            )

            # Generate verdict (includes merge conflict check and branch-behind check)
            verdict, verdict_reasoning, blockers = self._generate_verdict(
                unique_findings,
                has_merge_conflicts=context.has_merge_conflicts,
                merge_state_status=context.merge_state_status,
            )

            # Generate summary
            summary = self._generate_summary(
                verdict=verdict,
                verdict_reasoning=verdict_reasoning,
                blockers=blockers,
                findings=unique_findings,
                agents_invoked=final_agents,
            )

            # Map verdict to overall_status
            if verdict == MergeVerdict.BLOCKED:
                overall_status = "request_changes"
            elif verdict == MergeVerdict.NEEDS_REVISION:
                overall_status = "request_changes"
            elif verdict == MergeVerdict.MERGE_WITH_CHANGES:
                overall_status = "comment"
            else:
                overall_status = "approve"

            # Extract HEAD SHA from commits for follow-up review tracking
            head_sha = None
            if context.commits:
                latest_commit = context.commits[-1]
                head_sha = latest_commit.get("oid") or latest_commit.get("sha")

            # Get file blob SHAs for rebase-resistant follow-up reviews
            # Blob SHAs persist across rebases - same content = same blob SHA
            file_blobs: dict[str, str] = {}
            try:
                gh_client = GHClient(
                    project_dir=self.project_dir,
                    default_timeout=30.0,
                    repo=self.config.repo,
                )
                pr_files = await gh_client.get_pr_files(context.pr_number)
                for file in pr_files:
                    filename = file.get("filename", "")
                    blob_sha = file.get("sha", "")
                    if filename and blob_sha:
                        file_blobs[filename] = blob_sha
                logger.info(
                    f"Captured {len(file_blobs)} file blob SHAs for follow-up tracking"
                )
            except Exception as e:
                logger.warning(f"Could not capture file blobs: {e}")

            result = PRReviewResult(
                pr_number=context.pr_number,
                repo=self.config.repo,
                success=True,
                findings=unique_findings,
                summary=summary,
                overall_status=overall_status,
                verdict=verdict,
                verdict_reasoning=verdict_reasoning,
                blockers=blockers,
                reviewed_commit_sha=head_sha,
                reviewed_file_blobs=file_blobs,
            )

            self._report_progress(
                "analyzed",
                60,
                "Parallel analysis complete",
                pr_number=context.pr_number,
            )

            return result

        except Exception as e:
            logger.error(f"[ParallelOrchestrator] Review failed: {e}", exc_info=True)
            return PRReviewResult(
                pr_number=context.pr_number,
                repo=self.config.repo,
                success=False,
                error=str(e),
            )
        finally:
            # Always cleanup worktree, even on error
            if worktree_path:
                self._cleanup_pr_worktree(worktree_path)

    def _parse_structured_output(
        self, structured_output: dict[str, Any]
    ) -> tuple[list[PRReviewFinding] | None, list[str]]:
        """Parse findings and agents from SDK structured output.

        Returns:
            Tuple of (findings list or None if parsing failed, agents list)
        """
        findings = []
        agents_from_output: list[str] = []

        try:
            result = ParallelOrchestratorResponse.model_validate(structured_output)
            agents_from_output = result.agents_invoked or []

            logger.info(
                f"[ParallelOrchestrator] Structured output: verdict={result.verdict}, "
                f"{len(result.findings)} findings, agents={agents_from_output}"
            )

            # Log agents invoked with clear formatting
            self._log_agents_invoked(agents_from_output)

            # Convert structured findings to PRReviewFinding objects
            for f in result.findings:
                finding = self._create_finding_from_structured(f)
                findings.append(finding)

            # Log findings summary for verification
            self._log_findings_summary(findings)

        except Exception as e:
            logger.error(
                f"[ParallelOrchestrator] Structured output parsing failed: {e}"
            )
            return None, agents_from_output

        return findings, agents_from_output

    def _extract_json_from_text(self, output: str) -> dict[str, Any] | None:
        """Extract JSON object from text output.

        Args:
            output: Text output to parse

        Returns:
            Parsed JSON dict or None if not found
        """
        import json
        import re

        # Try to find JSON in code blocks
        code_block_pattern = r"```(?:json)?\s*(\{[\s\S]*?\})\s*```"
        code_block_match = re.search(code_block_pattern, output)

        if code_block_match:
            json_str = code_block_match.group(1)
            return json.loads(json_str)

        # Try to find raw JSON object
        start = output.find("{")
        if start == -1:
            return None

        brace_count = 0
        end = -1
        for i in range(start, len(output)):
            if output[i] == "{":
                brace_count += 1
            elif output[i] == "}":
                brace_count -= 1
                if brace_count == 0:
                    end = i
                    break

        if end != -1:
            json_str = output[start : end + 1]
            return json.loads(json_str)

        return None

    def _create_finding_from_dict(self, f_data: dict[str, Any]) -> PRReviewFinding:
        """Create a PRReviewFinding from dictionary data.

        Args:
            f_data: Finding data as dictionary

        Returns:
            PRReviewFinding instance
        """
        finding_id = hashlib.md5(
            f"{f_data.get('file', 'unknown')}:{f_data.get('line', 0)}:{f_data.get('title', 'Untitled')}".encode(),
            usedforsecurity=False,
        ).hexdigest()[:12]

        category = map_category(f_data.get("category", "quality"))

        try:
            severity = ReviewSeverity(f_data.get("severity", "medium").lower())
        except ValueError:
            severity = ReviewSeverity.MEDIUM

        return PRReviewFinding(
            id=finding_id,
            file=f_data.get("file", "unknown"),
            line=f_data.get("line", 0),
            title=f_data.get("title", "Untitled"),
            description=f_data.get("description", ""),
            category=category,
            severity=severity,
            suggested_fix=f_data.get("suggested_fix", ""),
            evidence=f_data.get("evidence"),
        )

    def _parse_text_output(self, output: str) -> list[PRReviewFinding]:
        """Parse findings from text output (fallback)."""
        findings = []

        try:
            # Extract JSON from text
            data = self._extract_json_from_text(output)
            if not data:
                return findings

            # Get findings array from JSON
            findings_data = data.get("findings", [])

            # Convert each finding dict to PRReviewFinding
            for f_data in findings_data:
                finding = self._create_finding_from_dict(f_data)
                findings.append(finding)

        except Exception as e:
            logger.error(f"[ParallelOrchestrator] Text parsing failed: {e}")

        return findings

    def _normalize_confidence(self, value: int | float) -> float:
        """Normalize confidence to 0.0-1.0 range."""
        if value > 1:
            return value / 100.0
        return float(value)

    def _deduplicate_findings(
        self, findings: list[PRReviewFinding]
    ) -> list[PRReviewFinding]:
        """Remove duplicate findings."""
        seen = set()
        unique = []

        for f in findings:
            key = (f.file, f.line, f.title.lower().strip())
            if key not in seen:
                seen.add(key)
                unique.append(f)

        return unique

    def _cross_validate_findings(
        self, findings: list[PRReviewFinding]
    ) -> tuple[list[PRReviewFinding], AgentAgreement]:
        """
        Cross-validate findings to boost confidence when multiple agents agree.

        Groups findings by location key (file, line, category) and:
        - For groups with 2+ findings: merges into one, boosts confidence by 0.15,
          sets cross_validated=True, collects all source agents
        - For single-agent findings: keeps as-is, ensures source_agents is populated

        Args:
            findings: List of deduplicated findings to cross-validate

        Returns:
            Tuple of (cross-validated findings, AgentAgreement tracking object)
        """
        # Confidence boost for multi-agent agreement
        CONFIDENCE_BOOST = 0.15
        MAX_CONFIDENCE = 0.95

        # Group findings by location key: (file, line, category)
        groups: dict[tuple, list[PRReviewFinding]] = defaultdict(list)
        for finding in findings:
            key = (finding.file, finding.line, finding.category.value)
            groups[key].append(finding)

        validated_findings: list[PRReviewFinding] = []
        agreed_finding_ids: list[str] = []

        for key, group in groups.items():
            if len(group) >= 2:
                # Multi-agent agreement: merge findings
                # Sort by severity to keep highest severity finding
                severity_order = {
                    ReviewSeverity.CRITICAL: 0,
                    ReviewSeverity.HIGH: 1,
                    ReviewSeverity.MEDIUM: 2,
                    ReviewSeverity.LOW: 3,
                }
                group.sort(key=lambda f: severity_order.get(f.severity, 99))
                primary = group[0]

                # Collect all source agents from group
                all_agents: list[str] = []
                for f in group:
                    if f.source_agents:
                        for agent in f.source_agents:
                            if agent not in all_agents:
                                all_agents.append(agent)

                # Combine evidence from all findings
                all_evidence: list[str] = []
                for f in group:
                    if f.evidence and f.evidence.strip():
                        all_evidence.append(f.evidence.strip())
                combined_evidence = (
                    "\n---\n".join(all_evidence) if all_evidence else None
                )

                # Combine descriptions
                all_descriptions: list[str] = [primary.description]
                for f in group[1:]:
                    if f.description and f.description not in all_descriptions:
                        all_descriptions.append(f.description)
                combined_description = " | ".join(all_descriptions)

                # Boost confidence (capped at MAX_CONFIDENCE)
                base_confidence = primary.confidence or 0.5
                boosted_confidence = min(
                    base_confidence + CONFIDENCE_BOOST, MAX_CONFIDENCE
                )

                # Update the primary finding with merged data
                primary.confidence = boosted_confidence
                primary.cross_validated = True
                primary.source_agents = all_agents
                primary.evidence = combined_evidence
                primary.description = combined_description

                validated_findings.append(primary)
                agreed_finding_ids.append(primary.id)

                logger.debug(
                    f"[PRReview] Cross-validated finding {primary.id}: "
                    f"merged {len(group)} findings, agents={all_agents}, "
                    f"confidence={boosted_confidence:.2f}"
                )
            else:
                # Single-agent finding: keep as-is
                finding = group[0]

                # Ensure source_agents is populated (use empty list if not set)
                if not finding.source_agents:
                    finding.source_agents = []

                validated_findings.append(finding)

        # Create agent agreement tracking object
        agent_agreement = AgentAgreement(
            agreed_findings=agreed_finding_ids,
            conflicting_findings=[],  # Not implemented yet - reserved for future
            resolution_notes=None,
        )

        return validated_findings, agent_agreement

    def _apply_confidence_routing(
        self, findings: list[PRReviewFinding]
    ) -> list[PRReviewFinding]:
        """
        Route findings based on confidence scores.

        - HIGH (>=0.8): Keep as-is, include in output
        - MEDIUM (0.5-0.8): Prepend "[Potential] " to title, include in output
        - LOW (<0.5): Log with logger.info(), exclude from output

        Args:
            findings: List of findings to route

        Returns:
            Filtered list of findings (HIGH and MEDIUM only)
        """
        routed = []
        tier_counts = {"high": 0, "medium": 0, "low": 0}

        for finding in findings:
            # Handle missing confidence gracefully (default to 0.5)
            confidence = getattr(finding, "confidence", 0.5)
            if confidence is None:
                confidence = 0.5
            confidence = self._normalize_confidence(confidence)

            tier = ConfidenceTier.get_tier(confidence)
            tier_counts[tier] += 1

            if tier == ConfidenceTier.HIGH:
                # HIGH: Include as-is
                routed.append(finding)
            elif tier == ConfidenceTier.MEDIUM:
                # MEDIUM: Prepend "[Potential] " to title
                if not finding.title.startswith("[Potential] "):
                    finding.title = f"[Potential] {finding.title}"
                routed.append(finding)
            else:
                # LOW: Log and exclude
                logger.info(
                    f"[PRReview] Dropping low-confidence finding: "
                    f"'{finding.title}' (confidence={confidence:.2f}, "
                    f"file={finding.file}:{finding.line})"
                )

        logger.info(
            f"[PRReview] Confidence routing: HIGH={tier_counts['high']}, "
            f"MEDIUM={tier_counts['medium']}, LOW={tier_counts['low']} "
            f"(dropped {tier_counts['low']} low-confidence findings)"
        )

        return routed

    def _generate_verdict(
        self,
        findings: list[PRReviewFinding],
        has_merge_conflicts: bool = False,
        merge_state_status: str = "",
    ) -> tuple[MergeVerdict, str, list[str]]:
        """Generate merge verdict based on findings, merge conflict status, and branch state."""
        blockers = []
        is_branch_behind = merge_state_status == "BEHIND"

        # CRITICAL: Merge conflicts block merging - check first
        if has_merge_conflicts:
            blockers.append(
                "Merge Conflicts: PR has conflicts with base branch that must be resolved"
            )
        # Branch behind base is a warning, not a hard blocker
        elif is_branch_behind:
            blockers.append(BRANCH_BEHIND_BLOCKER_MSG)

        critical = [f for f in findings if f.severity == ReviewSeverity.CRITICAL]
        high = [f for f in findings if f.severity == ReviewSeverity.HIGH]
        medium = [f for f in findings if f.severity == ReviewSeverity.MEDIUM]
        low = [f for f in findings if f.severity == ReviewSeverity.LOW]

        for f in critical:
            blockers.append(f"Critical: {f.title} ({f.file}:{f.line})")

        if blockers:
            # Merge conflicts are the highest priority blocker
            if has_merge_conflicts:
                verdict = MergeVerdict.BLOCKED
                reasoning = (
                    "Blocked: PR has merge conflicts with base branch. "
                    "Resolve conflicts before merge."
                )
            elif critical:
                verdict = MergeVerdict.BLOCKED
                reasoning = f"Blocked by {len(critical)} critical issue(s)"
            # Branch behind is a soft blocker - NEEDS_REVISION, not BLOCKED
            elif is_branch_behind:
                verdict = MergeVerdict.NEEDS_REVISION
                if high or medium:
                    # Branch behind + code issues that need addressing
                    total = len(high) + len(medium)
                    reasoning = (
                        f"{BRANCH_BEHIND_REASONING} "
                        f"{total} issue(s) must be addressed ({len(high)} required, {len(medium)} recommended)."
                    )
                else:
                    # Just branch behind, no code issues
                    reasoning = BRANCH_BEHIND_REASONING
                if low:
                    reasoning += f" {len(low)} non-blocking suggestion(s) to consider."
            else:
                verdict = MergeVerdict.BLOCKED
                reasoning = f"Blocked by {len(blockers)} issue(s)"
        elif high or medium:
            # High and Medium severity findings block merge
            verdict = MergeVerdict.NEEDS_REVISION
            total = len(high) + len(medium)
            reasoning = f"{total} issue(s) must be addressed ({len(high)} required, {len(medium)} recommended)"
            if low:
                reasoning += f", {len(low)} suggestions"
        elif low:
            # Only Low severity suggestions - safe to merge (non-blocking)
            verdict = MergeVerdict.READY_TO_MERGE
            reasoning = (
                f"No blocking issues. {len(low)} non-blocking suggestion(s) to consider"
            )
        else:
            verdict = MergeVerdict.READY_TO_MERGE
            reasoning = "No blocking issues found"

        return verdict, reasoning, blockers

    def _generate_summary(
        self,
        verdict: MergeVerdict,
        verdict_reasoning: str,
        blockers: list[str],
        findings: list[PRReviewFinding],
        agents_invoked: list[str],
    ) -> str:
        """Generate PR review summary."""
        verdict_emoji = {
            MergeVerdict.READY_TO_MERGE: "âœ…",
            MergeVerdict.MERGE_WITH_CHANGES: "ðŸŸ¡",
            MergeVerdict.NEEDS_REVISION: "ðŸŸ ",
            MergeVerdict.BLOCKED: "ðŸ”´",
        }

        lines = [
            f"### Merge Verdict: {verdict_emoji.get(verdict, 'âšª')} {verdict.value.upper().replace('_', ' ')}",
            verdict_reasoning,
            "",
        ]

        # Agents used
        if agents_invoked:
            lines.append(f"**Specialist Agents Invoked:** {', '.join(agents_invoked)}")
            lines.append("")

        # Blockers
        if blockers:
            lines.append("### ðŸš¨ Blocking Issues")
            for blocker in blockers:
                lines.append(f"- {blocker}")
            lines.append("")

        # Findings summary
        if findings:
            by_severity: dict[str, list] = {}
            for f in findings:
                severity = f.severity.value
                if severity not in by_severity:
                    by_severity[severity] = []
                by_severity[severity].append(f)

            lines.append("### Findings Summary")
            for severity in ["critical", "high", "medium", "low"]:
                if severity in by_severity:
                    count = len(by_severity[severity])
                    lines.append(f"- **{severity.capitalize()}**: {count} issue(s)")
            lines.append("")

        lines.append("---")
        lines.append("_Generated by Auto Claude Parallel Orchestrator (SDK Subagents)_")

        return "\n".join(lines)
