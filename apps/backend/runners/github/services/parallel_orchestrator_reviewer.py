"""
Parallel Orchestrator PR Reviewer
==================================

PR reviewer using Claude Agent SDK subagents for parallel specialist analysis.

The orchestrator analyzes the PR and delegates to specialized agents (security,
quality, logic, codebase-fit, ai-triage) which run in parallel. Results are
synthesized into a final verdict.

Key Design:
- AI decides which agents to invoke (NOT programmatic rules)
- Subagents defined via SDK `agents={}` parameter
- SDK handles parallel execution automatically
- User-configured model from frontend settings (no hardcoding)
"""

from __future__ import annotations

import hashlib
import logging
import os
import shutil
import subprocess
import uuid
from pathlib import Path
from typing import Any

from claude_agent_sdk import AgentDefinition
from core.git_bash import get_git_executable_path

try:
    from ...core.client import create_client
    from ...phase_config import get_thinking_budget
    from ..context_gatherer import PRContext, _validate_git_ref
    from ..gh_client import GHClient
    from ..models import (
        GitHubRunnerConfig,
        MergeVerdict,
        PRReviewFinding,
        PRReviewResult,
        ReviewSeverity,
    )
    from .category_utils import map_category
    from .pydantic_models import ParallelOrchestratorResponse
    from .sdk_utils import process_sdk_stream
except (ImportError, ValueError, SystemError):
    from context_gatherer import PRContext, _validate_git_ref
    from core.client import create_client
    from gh_client import GHClient
    from models import (
        GitHubRunnerConfig,
        MergeVerdict,
        PRReviewFinding,
        PRReviewResult,
        ReviewSeverity,
    )
    from phase_config import get_thinking_budget
    from services.category_utils import map_category
    from services.pydantic_models import ParallelOrchestratorResponse
    from services.sdk_utils import process_sdk_stream


logger = logging.getLogger(__name__)

# Check if debug mode is enabled
DEBUG_MODE = os.environ.get("DEBUG", "").lower() in ("true", "1", "yes")

# Directory for PR review worktrees (inside github/pr for consistency)
PR_WORKTREE_DIR = ".auto-claude/github/pr/worktrees"


class ParallelOrchestratorReviewer:
    """
    PR reviewer using SDK subagents for parallel specialist analysis.

    The orchestrator:
    1. Analyzes the PR (size, complexity, file types, risk areas)
    2. Delegates to appropriate specialist agents (SDK handles parallel execution)
    3. Synthesizes findings into a final verdict

    Model Configuration:
    - Orchestrator uses user-configured model from frontend settings
    - Specialist agents use model="inherit" (same as orchestrator)
    """

    def __init__(
        self,
        project_dir: Path,
        github_dir: Path,
        config: GitHubRunnerConfig,
        progress_callback=None,
    ):
        self.project_dir = Path(project_dir)
        self.github_dir = Path(github_dir)
        self.config = config
        self.progress_callback = progress_callback

    def _report_progress(self, phase: str, progress: int, message: str, **kwargs):
        """Report progress if callback is set."""
        if self.progress_callback:
            import sys

            if "orchestrator" in sys.modules:
                ProgressCallback = sys.modules["orchestrator"].ProgressCallback
            else:
                try:
                    from ..orchestrator import ProgressCallback
                except ImportError:
                    from orchestrator import ProgressCallback

            self.progress_callback(
                ProgressCallback(
                    phase=phase, progress=progress, message=message, **kwargs
                )
            )

    def _load_prompt(self, filename: str) -> str:
        """Load a prompt file from the prompts/github directory."""
        prompt_file = (
            Path(__file__).parent.parent.parent.parent / "prompts" / "github" / filename
        )
        if prompt_file.exists():
            return prompt_file.read_text(encoding="utf-8")
        logger.warning(f"Prompt file not found: {prompt_file}")
        return ""

    def _create_pr_worktree(self, head_sha: str, pr_number: int) -> Path:
        """Create a temporary worktree at the PR head commit.

        Args:
            head_sha: The commit SHA of the PR head (validated before use)
            pr_number: The PR number for naming

        Returns:
            Path to the created worktree

        Raises:
            RuntimeError: If worktree creation fails
            ValueError: If head_sha fails validation (command injection prevention)
        """
        # SECURITY: Validate git ref before use in subprocess calls
        if not _validate_git_ref(head_sha):
            raise ValueError(
                f"Invalid git ref: '{head_sha}'. "
                "Must contain only alphanumeric characters, dots, slashes, underscores, and hyphens."
            )

        worktree_name = f"pr-{pr_number}-{uuid.uuid4().hex[:8]}"
        worktree_dir = self.project_dir / PR_WORKTREE_DIR

        if DEBUG_MODE:
            print(f"[PRReview] DEBUG: project_dir={self.project_dir}", flush=True)
            print(f"[PRReview] DEBUG: worktree_dir={worktree_dir}", flush=True)
            print(f"[PRReview] DEBUG: head_sha={head_sha}", flush=True)

        worktree_dir.mkdir(parents=True, exist_ok=True)
        worktree_path = worktree_dir / worktree_name

        if DEBUG_MODE:
            print(f"[PRReview] DEBUG: worktree_path={worktree_path}", flush=True)
            print(
                f"[PRReview] DEBUG: worktree_dir exists={worktree_dir.exists()}",
                flush=True,
            )

        # Fetch the commit if not available locally (handles fork PRs)
        git_path = get_git_executable_path()
        fetch_result = subprocess.run(
            [git_path, "fetch", "origin", head_sha],
            cwd=self.project_dir,
            capture_output=True,
            text=True,
            timeout=60,
        )
        if DEBUG_MODE:
            print(
                f"[PRReview] DEBUG: fetch returncode={fetch_result.returncode}",
                flush=True,
            )
            if fetch_result.stderr:
                print(
                    f"[PRReview] DEBUG: fetch stderr={fetch_result.stderr[:200]}",
                    flush=True,
                )

        # Create detached worktree at the PR commit
        result = subprocess.run(
            [git_path, "worktree", "add", "--detach", str(worktree_path), head_sha],
            cwd=self.project_dir,
            capture_output=True,
            text=True,
            timeout=120,  # Worktree add can be slow for large repos
        )

        if DEBUG_MODE:
            print(
                f"[PRReview] DEBUG: worktree add returncode={result.returncode}",
                flush=True,
            )
            if result.stderr:
                print(
                    f"[PRReview] DEBUG: worktree add stderr={result.stderr[:200]}",
                    flush=True,
                )
            if result.stdout:
                print(
                    f"[PRReview] DEBUG: worktree add stdout={result.stdout[:200]}",
                    flush=True,
                )

        if result.returncode != 0:
            raise RuntimeError(f"Failed to create worktree: {result.stderr}")

        if DEBUG_MODE:
            print(
                f"[PRReview] DEBUG: worktree created, exists={worktree_path.exists()}",
                flush=True,
            )
        logger.info(f"[PRReview] Created worktree at {worktree_path}")
        return worktree_path

    def _cleanup_pr_worktree(self, worktree_path: Path) -> None:
        """Remove a temporary PR review worktree with fallback chain.

        Args:
            worktree_path: Path to the worktree to remove
        """
        if DEBUG_MODE:
            print(
                f"[PRReview] DEBUG: _cleanup_pr_worktree called with {worktree_path}",
                flush=True,
            )

        if not worktree_path or not worktree_path.exists():
            if DEBUG_MODE:
                print(
                    "[PRReview] DEBUG: worktree path doesn't exist, skipping cleanup",
                    flush=True,
                )
            return

        if DEBUG_MODE:
            print(
                f"[PRReview] DEBUG: Attempting to remove worktree at {worktree_path}",
                flush=True,
            )

        # Try 1: git worktree remove
        git_path = get_git_executable_path()
        result = subprocess.run(
            [git_path, "worktree", "remove", "--force", str(worktree_path)],
            cwd=self.project_dir,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if DEBUG_MODE:
            print(
                f"[PRReview] DEBUG: worktree remove returncode={result.returncode}",
                flush=True,
            )

        if result.returncode == 0:
            logger.info(f"[PRReview] Cleaned up worktree: {worktree_path.name}")
            return

        # Try 2: shutil.rmtree fallback
        try:
            shutil.rmtree(worktree_path, ignore_errors=True)
            subprocess.run(
                [git_path, "worktree", "prune"],
                cwd=self.project_dir,
                capture_output=True,
                timeout=30,
            )
            logger.warning(f"[PRReview] Used shutil fallback for: {worktree_path.name}")
        except Exception as e:
            logger.error(f"[PRReview] Failed to cleanup worktree {worktree_path}: {e}")

    def _cleanup_stale_pr_worktrees(self) -> None:
        """Clean up orphaned PR review worktrees on startup."""
        worktree_dir = self.project_dir / PR_WORKTREE_DIR
        if not worktree_dir.exists():
            return

        # Get registered worktrees from git
        git_path = get_git_executable_path()
        result = subprocess.run(
            [git_path, "worktree", "list", "--porcelain"],
            cwd=self.project_dir,
            capture_output=True,
            text=True,
            timeout=30,
        )
        registered = set()
        for line in result.stdout.split("\n"):
            if line.startswith("worktree "):
                # Safely parse - check bounds to prevent IndexError
                parts = line.split(" ", 1)
                if len(parts) > 1 and parts[1]:
                    registered.add(Path(parts[1]))

        # Remove unregistered directories
        stale_count = 0
        for item in worktree_dir.iterdir():
            if item.is_dir() and item not in registered:
                logger.info(f"[PRReview] Removing stale worktree: {item.name}")
                shutil.rmtree(item, ignore_errors=True)
                stale_count += 1

        if stale_count > 0:
            subprocess.run(
                [git_path, "worktree", "prune"],
                cwd=self.project_dir,
                capture_output=True,
                timeout=30,
            )
            if DEBUG_MODE:
                print(
                    f"[PRReview] DEBUG: Cleaned up {stale_count} stale worktree(s)",
                    flush=True,
                )

    def _define_specialist_agents(self) -> dict[str, AgentDefinition]:
        """
        Define specialist agents for the SDK.

        Each agent has:
        - description: When the orchestrator should invoke this agent
        - prompt: System prompt for the agent
        - tools: Tools the agent can use (read-only for PR review)
        - model: "inherit" = use same model as orchestrator (user's choice)

        Returns AgentDefinition dataclass instances as required by the SDK.
        """
        # Load agent prompts from files
        security_prompt = self._load_prompt("pr_security_agent.md")
        quality_prompt = self._load_prompt("pr_quality_agent.md")
        logic_prompt = self._load_prompt("pr_logic_agent.md")
        codebase_fit_prompt = self._load_prompt("pr_codebase_fit_agent.md")
        ai_triage_prompt = self._load_prompt("pr_ai_triage.md")

        return {
            "security-reviewer": AgentDefinition(
                description=(
                    "Security specialist. Use for OWASP Top 10, authentication, "
                    "injection, cryptographic issues, and sensitive data exposure. "
                    "Invoke when PR touches auth, API endpoints, user input, database queries, "
                    "or file operations."
                ),
                prompt=security_prompt
                or "You are a security expert. Find vulnerabilities.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "quality-reviewer": AgentDefinition(
                description=(
                    "Code quality expert. Use for complexity, duplication, error handling, "
                    "maintainability, and pattern adherence. Invoke when PR has complex logic, "
                    "large functions, or significant business logic changes."
                ),
                prompt=quality_prompt
                or "You are a code quality expert. Find quality issues.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "logic-reviewer": AgentDefinition(
                description=(
                    "Logic and correctness specialist. Use for algorithm verification, "
                    "edge cases, state management, and race conditions. Invoke when PR has "
                    "algorithmic changes, data transformations, concurrent operations, or bug fixes."
                ),
                prompt=logic_prompt
                or "You are a logic expert. Find correctness issues.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "codebase-fit-reviewer": AgentDefinition(
                description=(
                    "Codebase consistency expert. Use for naming conventions, ecosystem fit, "
                    "architectural alignment, and avoiding reinvention. Invoke when PR introduces "
                    "new patterns, large additions, or code that might duplicate existing functionality."
                ),
                prompt=codebase_fit_prompt
                or "You are a codebase expert. Check for consistency.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "ai-triage-reviewer": AgentDefinition(
                description=(
                    "AI comment validator. Use for triaging comments from CodeRabbit, "
                    "Gemini Code Assist, Cursor, Greptile, and other AI reviewers. "
                    "Invoke when PR has existing AI review comments that need validation."
                ),
                prompt=ai_triage_prompt
                or "You are an AI triage expert. Validate AI comments.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
        }

    def _build_orchestrator_prompt(self, context: PRContext) -> str:
        """Build full prompt for orchestrator with PR context."""
        # Load orchestrator prompt
        base_prompt = self._load_prompt("pr_parallel_orchestrator.md")
        if not base_prompt:
            base_prompt = "You are a PR reviewer. Analyze and delegate to specialists."

        # Build file list
        files_list = []
        for file in context.changed_files:
            files_list.append(
                f"- `{file.path}` (+{file.additions}/-{file.deletions}) - {file.status}"
            )

        # Build composite diff
        patches = []
        MAX_DIFF_CHARS = 200_000

        for file in context.changed_files:
            if file.patch:
                patches.append(f"\n### File: {file.path}\n{file.patch}")

        diff_content = "\n".join(patches)

        if len(diff_content) > MAX_DIFF_CHARS:
            diff_content = diff_content[:MAX_DIFF_CHARS] + "\n\n... (diff truncated)"

        # Build AI comments context if present
        ai_comments_section = ""
        if context.ai_bot_comments:
            ai_comments_list = []
            for comment in context.ai_bot_comments[:20]:
                ai_comments_list.append(
                    f"- **{comment.tool_name}** on {comment.file or 'general'}: "
                    f"{comment.body[:200]}..."
                )
            ai_comments_section = f"""
### AI Review Comments (need triage)
Found {len(context.ai_bot_comments)} comments from AI tools:
{chr(10).join(ai_comments_list)}
"""

        pr_context = f"""
---

## PR Context for Review

**PR Number:** {context.pr_number}
**Title:** {context.title}
**Author:** {context.author}
**Base:** {context.base_branch} â† **Head:** {context.head_branch}
**Files Changed:** {len(context.changed_files)} files
**Total Changes:** +{context.total_additions}/-{context.total_deletions} lines

### Description
{context.description}

### All Changed Files
{chr(10).join(files_list)}
{ai_comments_section}
### Code Changes
```diff
{diff_content}
```

---

Now analyze this PR and delegate to the appropriate specialist agents.
Remember: YOU decide which agents to invoke based on YOUR analysis.
The SDK will run invoked agents in parallel automatically.
"""

        return base_prompt + pr_context

    def _create_sdk_client(
        self, project_root: Path, model: str, thinking_budget: int | None
    ):
        """Create SDK client with subagents and configuration.

        Args:
            project_root: Root directory of the project
            model: Model to use for orchestrator
            thinking_budget: Max thinking tokens budget

        Returns:
            Configured SDK client instance
        """
        return create_client(
            project_dir=project_root,
            spec_dir=self.github_dir,
            model=model,
            agent_type="pr_orchestrator_parallel",
            max_thinking_tokens=thinking_budget,
            agents=self._define_specialist_agents(),
            output_format={
                "type": "json_schema",
                "schema": ParallelOrchestratorResponse.model_json_schema(),
            },
        )

    def _extract_structured_output(
        self, structured_output: dict[str, Any] | None, result_text: str
    ) -> tuple[list[PRReviewFinding], list[str]]:
        """Parse and extract findings from structured output or text fallback.

        Args:
            structured_output: Structured JSON output from agent
            result_text: Raw text output as fallback

        Returns:
            Tuple of (findings list, agents_invoked list)
        """
        agents_from_structured: list[str] = []

        if structured_output:
            findings, agents_from_structured = self._parse_structured_output(
                structured_output
            )
            if findings is None and result_text:
                findings = self._parse_text_output(result_text)
            elif findings is None:
                findings = []
        else:
            findings = self._parse_text_output(result_text)

        return findings, agents_from_structured

    def _log_agents_invoked(self, agents: list[str]) -> None:
        """Log invoked agents with clear formatting.

        Args:
            agents: List of agent names that were invoked
        """
        if agents:
            print(
                f"[ParallelOrchestrator] Specialist agents invoked: {', '.join(agents)}",
                flush=True,
            )
            for agent in agents:
                print(f"[Agent:{agent}] Analysis complete", flush=True)

    def _log_findings_summary(self, findings: list[PRReviewFinding]) -> None:
        """Log findings summary for verification.

        Args:
            findings: List of findings to summarize
        """
        if findings:
            print(
                f"[ParallelOrchestrator] Parsed {len(findings)} findings from structured output",
                flush=True,
            )
            print("[ParallelOrchestrator] Findings summary:", flush=True)
            for i, f in enumerate(findings, 1):
                print(
                    f"  [{f.severity.value.upper()}] {i}. {f.title} ({f.file}:{f.line})",
                    flush=True,
                )

    def _create_finding_from_structured(self, finding_data: Any) -> PRReviewFinding:
        """Create a PRReviewFinding from structured output data.

        Args:
            finding_data: Finding data from structured output

        Returns:
            PRReviewFinding instance
        """
        finding_id = hashlib.md5(
            f"{finding_data.file}:{finding_data.line}:{finding_data.title}".encode(),
            usedforsecurity=False,
        ).hexdigest()[:12]

        category = map_category(finding_data.category)

        try:
            severity = ReviewSeverity(finding_data.severity.lower())
        except ValueError:
            severity = ReviewSeverity.MEDIUM

        return PRReviewFinding(
            id=finding_id,
            file=finding_data.file,
            line=finding_data.line,
            title=finding_data.title,
            description=finding_data.description,
            category=category,
            severity=severity,
            suggested_fix=finding_data.suggested_fix or "",
            evidence=finding_data.evidence,
        )

    async def review(self, context: PRContext) -> PRReviewResult:
        """
        Main review entry point.

        Args:
            context: Full PR context with all files and patches

        Returns:
            PRReviewResult with findings and verdict
        """
        logger.info(
            f"[ParallelOrchestrator] Starting review for PR #{context.pr_number}"
        )

        # Clean up any stale worktrees from previous runs
        self._cleanup_stale_pr_worktrees()

        # Track worktree for cleanup
        worktree_path: Path | None = None

        try:
            self._report_progress(
                "orchestrating",
                35,
                "Parallel orchestrator analyzing PR...",
                pr_number=context.pr_number,
            )

            # Build orchestrator prompt
            prompt = self._build_orchestrator_prompt(context)

            # Create temporary worktree at PR head commit for isolated review
            # This ensures agents read from the correct PR state, not the current checkout
            head_sha = context.head_sha or context.head_branch

            if DEBUG_MODE:
                print(
                    f"[PRReview] DEBUG: context.head_sha='{context.head_sha}'",
                    flush=True,
                )
                print(
                    f"[PRReview] DEBUG: context.head_branch='{context.head_branch}'",
                    flush=True,
                )
                print(f"[PRReview] DEBUG: resolved head_sha='{head_sha}'", flush=True)

            # SECURITY: Validate the resolved head_sha (whether SHA or branch name)
            # This catches invalid refs early before subprocess calls
            if head_sha and not _validate_git_ref(head_sha):
                logger.warning(
                    f"[ParallelOrchestrator] Invalid git ref '{head_sha}', "
                    "using current checkout for safety"
                )
                head_sha = None

            if not head_sha:
                if DEBUG_MODE:
                    print("[PRReview] DEBUG: No head_sha - using fallback", flush=True)
                logger.warning(
                    "[ParallelOrchestrator] No head_sha available, using current checkout"
                )
                # Fallback to original behavior if no SHA available
                project_root = (
                    self.project_dir.parent.parent
                    if self.project_dir.name == "backend"
                    else self.project_dir
                )
            else:
                if DEBUG_MODE:
                    print(
                        f"[PRReview] DEBUG: Creating worktree for head_sha={head_sha}",
                        flush=True,
                    )
                try:
                    worktree_path = self._create_pr_worktree(
                        head_sha, context.pr_number
                    )
                    project_root = worktree_path
                    if DEBUG_MODE:
                        print(
                            f"[PRReview] DEBUG: Using worktree as "
                            f"project_root={project_root}",
                            flush=True,
                        )
                except (RuntimeError, ValueError) as e:
                    if DEBUG_MODE:
                        print(
                            f"[PRReview] DEBUG: Worktree creation FAILED: {e}",
                            flush=True,
                        )
                    logger.warning(
                        f"[ParallelOrchestrator] Worktree creation failed, "
                        f"using current checkout: {e}"
                    )
                    # Fallback to original behavior if worktree creation fails
                    project_root = (
                        self.project_dir.parent.parent
                        if self.project_dir.name == "backend"
                        else self.project_dir
                    )

            # Use model and thinking level from config (user settings)
            model = self.config.model or "claude-sonnet-4-5-20250929"
            thinking_level = self.config.thinking_level or "medium"
            thinking_budget = get_thinking_budget(thinking_level)

            logger.info(
                f"[ParallelOrchestrator] Using model={model}, "
                f"thinking_level={thinking_level}, thinking_budget={thinking_budget}"
            )

            # Create client with subagents defined
            # SDK handles parallel execution when Claude invokes multiple Task tools
            client = self._create_sdk_client(project_root, model, thinking_budget)

            self._report_progress(
                "orchestrating",
                40,
                "Orchestrator delegating to specialist agents...",
                pr_number=context.pr_number,
            )

            # Run orchestrator session using shared SDK stream processor
            async with client:
                await client.query(prompt)

                print(
                    f"[ParallelOrchestrator] Running orchestrator ({model})...",
                    flush=True,
                )

                # Process SDK stream with shared utility
                stream_result = await process_sdk_stream(
                    client=client,
                    context_name="ParallelOrchestrator",
                )

                # Check for stream processing errors
                if stream_result.get("error"):
                    logger.error(
                        f"[ParallelOrchestrator] SDK stream failed: {stream_result['error']}"
                    )
                    raise RuntimeError(
                        f"SDK stream processing failed: {stream_result['error']}"
                    )

                result_text = stream_result["result_text"]
                structured_output = stream_result["structured_output"]
                agents_invoked = stream_result["agents_invoked"]
                msg_count = stream_result["msg_count"]

            self._report_progress(
                "finalizing",
                50,
                "Synthesizing findings...",
                pr_number=context.pr_number,
            )

            # Parse findings from output (structured output also returns agents)
            findings, agents_from_structured = self._extract_structured_output(
                structured_output, result_text
            )

            # Use agents from structured output (more reliable than streaming detection)
            final_agents = (
                agents_from_structured if agents_from_structured else agents_invoked
            )
            logger.info(
                f"[ParallelOrchestrator] Session complete. Agents invoked: {final_agents}"
            )
            print(
                f"[ParallelOrchestrator] Complete. Agents invoked: {final_agents}",
                flush=True,
            )

            # Deduplicate findings
            unique_findings = self._deduplicate_findings(findings)

            logger.info(
                f"[ParallelOrchestrator] Review complete: {len(unique_findings)} findings"
            )

            # Generate verdict
            verdict, verdict_reasoning, blockers = self._generate_verdict(
                unique_findings
            )

            # Generate summary
            summary = self._generate_summary(
                verdict=verdict,
                verdict_reasoning=verdict_reasoning,
                blockers=blockers,
                findings=unique_findings,
                agents_invoked=final_agents,
            )

            # Map verdict to overall_status
            if verdict == MergeVerdict.BLOCKED:
                overall_status = "request_changes"
            elif verdict == MergeVerdict.NEEDS_REVISION:
                overall_status = "request_changes"
            elif verdict == MergeVerdict.MERGE_WITH_CHANGES:
                overall_status = "comment"
            else:
                overall_status = "approve"

            # Extract HEAD SHA from commits for follow-up review tracking
            head_sha = None
            if context.commits:
                latest_commit = context.commits[-1]
                head_sha = latest_commit.get("oid") or latest_commit.get("sha")

            # Get file blob SHAs for rebase-resistant follow-up reviews
            # Blob SHAs persist across rebases - same content = same blob SHA
            file_blobs: dict[str, str] = {}
            try:
                gh_client = GHClient(
                    project_dir=self.project_dir,
                    default_timeout=30.0,
                    repo=self.config.repo,
                )
                pr_files = await gh_client.get_pr_files(context.pr_number)
                for file in pr_files:
                    filename = file.get("filename", "")
                    blob_sha = file.get("sha", "")
                    if filename and blob_sha:
                        file_blobs[filename] = blob_sha
                logger.info(
                    f"Captured {len(file_blobs)} file blob SHAs for follow-up tracking"
                )
            except Exception as e:
                logger.warning(f"Could not capture file blobs: {e}")

            result = PRReviewResult(
                pr_number=context.pr_number,
                repo=self.config.repo,
                success=True,
                findings=unique_findings,
                summary=summary,
                overall_status=overall_status,
                verdict=verdict,
                verdict_reasoning=verdict_reasoning,
                blockers=blockers,
                reviewed_commit_sha=head_sha,
                reviewed_file_blobs=file_blobs,
            )

            self._report_progress(
                "analyzed",
                60,
                "Parallel analysis complete",
                pr_number=context.pr_number,
            )

            return result

        except Exception as e:
            logger.error(f"[ParallelOrchestrator] Review failed: {e}", exc_info=True)
            return PRReviewResult(
                pr_number=context.pr_number,
                repo=self.config.repo,
                success=False,
                error=str(e),
            )
        finally:
            # Always cleanup worktree, even on error
            if worktree_path:
                self._cleanup_pr_worktree(worktree_path)

    def _parse_structured_output(
        self, structured_output: dict[str, Any]
    ) -> tuple[list[PRReviewFinding] | None, list[str]]:
        """Parse findings and agents from SDK structured output.

        Returns:
            Tuple of (findings list or None if parsing failed, agents list)
        """
        findings = []
        agents_from_output: list[str] = []

        try:
            result = ParallelOrchestratorResponse.model_validate(structured_output)
            agents_from_output = result.agents_invoked or []

            logger.info(
                f"[ParallelOrchestrator] Structured output: verdict={result.verdict}, "
                f"{len(result.findings)} findings, agents={agents_from_output}"
            )

            # Log agents invoked with clear formatting
            self._log_agents_invoked(agents_from_output)

            # Convert structured findings to PRReviewFinding objects
            for f in result.findings:
                finding = self._create_finding_from_structured(f)
                findings.append(finding)

            # Log findings summary for verification
            self._log_findings_summary(findings)

        except Exception as e:
            logger.error(
                f"[ParallelOrchestrator] Structured output parsing failed: {e}"
            )
            return None, agents_from_output

        return findings, agents_from_output

    def _extract_json_from_text(self, output: str) -> dict[str, Any] | None:
        """Extract JSON object from text output.

        Args:
            output: Text output to parse

        Returns:
            Parsed JSON dict or None if not found
        """
        import json
        import re

        # Try to find JSON in code blocks
        code_block_pattern = r"```(?:json)?\s*(\{[\s\S]*?\})\s*```"
        code_block_match = re.search(code_block_pattern, output)

        if code_block_match:
            json_str = code_block_match.group(1)
            return json.loads(json_str)

        # Try to find raw JSON object
        start = output.find("{")
        if start == -1:
            return None

        brace_count = 0
        end = -1
        for i in range(start, len(output)):
            if output[i] == "{":
                brace_count += 1
            elif output[i] == "}":
                brace_count -= 1
                if brace_count == 0:
                    end = i
                    break

        if end != -1:
            json_str = output[start : end + 1]
            return json.loads(json_str)

        return None

    def _create_finding_from_dict(self, f_data: dict[str, Any]) -> PRReviewFinding:
        """Create a PRReviewFinding from dictionary data.

        Args:
            f_data: Finding data as dictionary

        Returns:
            PRReviewFinding instance
        """
        finding_id = hashlib.md5(
            f"{f_data.get('file', 'unknown')}:{f_data.get('line', 0)}:{f_data.get('title', 'Untitled')}".encode(),
            usedforsecurity=False,
        ).hexdigest()[:12]

        category = map_category(f_data.get("category", "quality"))

        try:
            severity = ReviewSeverity(f_data.get("severity", "medium").lower())
        except ValueError:
            severity = ReviewSeverity.MEDIUM

        return PRReviewFinding(
            id=finding_id,
            file=f_data.get("file", "unknown"),
            line=f_data.get("line", 0),
            title=f_data.get("title", "Untitled"),
            description=f_data.get("description", ""),
            category=category,
            severity=severity,
            suggested_fix=f_data.get("suggested_fix", ""),
            evidence=f_data.get("evidence"),
        )

    def _parse_text_output(self, output: str) -> list[PRReviewFinding]:
        """Parse findings from text output (fallback)."""
        findings = []

        try:
            # Extract JSON from text
            data = self._extract_json_from_text(output)
            if not data:
                return findings

            # Get findings array from JSON
            findings_data = data.get("findings", [])

            # Convert each finding dict to PRReviewFinding
            for f_data in findings_data:
                finding = self._create_finding_from_dict(f_data)
                findings.append(finding)

        except Exception as e:
            logger.error(f"[ParallelOrchestrator] Text parsing failed: {e}")

        return findings

    def _normalize_confidence(self, value: int | float) -> float:
        """Normalize confidence to 0.0-1.0 range."""
        if value > 1:
            return value / 100.0
        return float(value)

    def _deduplicate_findings(
        self, findings: list[PRReviewFinding]
    ) -> list[PRReviewFinding]:
        """Remove duplicate findings."""
        seen = set()
        unique = []

        for f in findings:
            key = (f.file, f.line, f.title.lower().strip())
            if key not in seen:
                seen.add(key)
                unique.append(f)

        return unique

    def _generate_verdict(
        self, findings: list[PRReviewFinding]
    ) -> tuple[MergeVerdict, str, list[str]]:
        """Generate merge verdict based on findings."""
        blockers = []

        critical = [f for f in findings if f.severity == ReviewSeverity.CRITICAL]
        high = [f for f in findings if f.severity == ReviewSeverity.HIGH]
        medium = [f for f in findings if f.severity == ReviewSeverity.MEDIUM]
        low = [f for f in findings if f.severity == ReviewSeverity.LOW]

        for f in critical:
            blockers.append(f"Critical: {f.title} ({f.file}:{f.line})")

        if blockers:
            verdict = MergeVerdict.BLOCKED
            reasoning = f"Blocked by {len(blockers)} critical issue(s)"
        elif high or medium:
            # High and Medium severity findings block merge
            verdict = MergeVerdict.NEEDS_REVISION
            total = len(high) + len(medium)
            reasoning = f"{total} issue(s) must be addressed ({len(high)} required, {len(medium)} recommended)"
            if low:
                reasoning += f", {len(low)} suggestions"
        elif low:
            # Only Low severity suggestions - safe to merge (non-blocking)
            verdict = MergeVerdict.READY_TO_MERGE
            reasoning = (
                f"No blocking issues. {len(low)} non-blocking suggestion(s) to consider"
            )
        else:
            verdict = MergeVerdict.READY_TO_MERGE
            reasoning = "No blocking issues found"

        return verdict, reasoning, blockers

    def _generate_summary(
        self,
        verdict: MergeVerdict,
        verdict_reasoning: str,
        blockers: list[str],
        findings: list[PRReviewFinding],
        agents_invoked: list[str],
    ) -> str:
        """Generate PR review summary."""
        verdict_emoji = {
            MergeVerdict.READY_TO_MERGE: "âœ…",
            MergeVerdict.MERGE_WITH_CHANGES: "ðŸŸ¡",
            MergeVerdict.NEEDS_REVISION: "ðŸŸ ",
            MergeVerdict.BLOCKED: "ðŸ”´",
        }

        lines = [
            f"### Merge Verdict: {verdict_emoji.get(verdict, 'âšª')} {verdict.value.upper().replace('_', ' ')}",
            verdict_reasoning,
            "",
        ]

        # Agents used
        if agents_invoked:
            lines.append(f"**Specialist Agents Invoked:** {', '.join(agents_invoked)}")
            lines.append("")

        # Blockers
        if blockers:
            lines.append("### ðŸš¨ Blocking Issues")
            for blocker in blockers:
                lines.append(f"- {blocker}")
            lines.append("")

        # Findings summary
        if findings:
            by_severity: dict[str, list] = {}
            for f in findings:
                severity = f.severity.value
                if severity not in by_severity:
                    by_severity[severity] = []
                by_severity[severity].append(f)

            lines.append("### Findings Summary")
            for severity in ["critical", "high", "medium", "low"]:
                if severity in by_severity:
                    count = len(by_severity[severity])
                    lines.append(f"- **{severity.capitalize()}**: {count} issue(s)")
            lines.append("")

        lines.append("---")
        lines.append("_Generated by Auto Claude Parallel Orchestrator (SDK Subagents)_")

        return "\n".join(lines)
